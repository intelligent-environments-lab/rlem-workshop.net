---
name: 'CityLearn: A Tutorial on Reinforcement Learning Control for Grid-Interactive Efficient Buildings and Communities'
speakers:
- Kingsley Nweye
categories:
- tutorial
permalink: /:collection/:categories/Tutorial 1
---

<h2>Description</h2>
<p>The <a href="https://colab.research.google.com/github/intelligent-environments-lab/CityLearn/blob/master/examples/citylearn_rlem23_tutorial.ipynb" target="_blank">CityLearn tutorial at RLEM'23</a> will help participants get acquainted with the <a href="https://www.citylearn.net" target="_blank">CityLearn</a> OpenAI Gym environment, developed for easy implementation and benchmarking of control algorithms, e.g., rule-based control, model predictive control or deep reinforcement learning control in the demand response, building energy and grid-interactive community domain. By the end of the tutorial, participants will learn how to design their own simple or advanced control algorithms to provide energy flexibility, and acquire familiarity with the CityLearn environment for extended use in personal projects.</p>

<h2>Learning Outcomes</h2>
<p>The primary learning outcome for participants is to gain familiarity with CityLearn environment, its application programming interface (API) and dataset offerings for extended use in academic research or personal projects. Other secondary outcomes are to:</p>

<ol>
    <li>Understand how electrification, distributed energy resources e.g. batteries, photovoltaic (PV) systems and smart controls provide a promising pathway to decarbonization and energy flexibility.</li>
    <li>Learn how to design and optimize their own rule-based control (RBC) agent for battery management using readily available knowledge of a building's energy use.</li>
    <li>Identify the challenges surrounding the generalizability of an RBC agent and how reinforcement learning (RL) can mitigate these challenges.</li>
    <li>Train their own RL Tabular Q-Learning algorithm.</li>
    <li>Evaluate the performance of a standard model-free deep RL algorithm in optimizing key performance indicators (KPIs) that are targeted at quantifying energy flexibility, environmental and economic costs.</li>
    <li>Learn the effect of different control algorithms and their parameters in improving these KPIs.</li>
    <li>Make submission to <a href="https://www.aicrowd.com/challenges/neurips-2023-citylearn-challenge" target="_blank">The CityLearn Challenge 2023</a>.</li>
</ol>

<h2>Target Audience</h2>
<p>The target audience for this tutorial includes the following:</p>

<ul>
    <li>Academic, private and commercial researchers or professionals that are interested in sustainable artificial intelligence (AI)-related pathways to electrification and building decarbonization.</li>
    <li>I enthusiasts with at least beginner level of expertise in programming or data science whom may have or pick interest in solving control problems or are interested in learning about a new reinforcement learning (RL) environment that deviates from popular simpler problems e.g. the Atari, MuJoCo and Box2D Gym environments, to real-world problems with urgency like climate change mitigation and decarbonization of the electric grid infrastructure and building end-uses.</li>
</ul>

<h2>Prerequisites</h2>

<p>The CityLearn tutorial has a fairly low entry level and participants do not need to have prior experience in reinforcement learning (RL) nor use of a Gym environment. However, participants need to have at least, beginner knowledge in Python or other similar high-level scripting language. Also, participants should have a computer that is able to launch a Google Colab notebook in the browser or a Jupyter notebook locally.</p>

<h2>Program</h2>
<p>The workshop is divided into the following parts (note that durations are flexible):</p>

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-1wig{font-weight:bold;text-align:left;vertical-align:top}
.tg .tg-fymr{border-color:inherit;font-weight:bold;text-align:left;vertical-align:top}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-0lax{text-align:left;vertical-align:top}
.tg .tg-hohi{color:#C9D1D9;text-align:right;vertical-align:top}
.tg .tg-hd9x{color:#C9D1D9;text-align:left;vertical-align:top}
</style>
<table class="tg">
    <thead>
        <tr>
            <th class="tg-fymr">Duration</th>
            <th class="tg-fymr">Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td class="tg-0pky">5m</td>
            <td class="tg-0pky">Overview, Learning Outcomes, Climate Impact, Target Audience and Prerequisites</td>
        </tr>
        <tr>
            <td class="tg-0pky">20m</td>
            <td class="tg-0pky">Background on Grid-Interactive Efficient Buildings, Energy Flexibility and CityLearn</td>
        </tr>
        <tr>
            <td class="tg-0pky">15m</td>
            <td class="tg-0pky">Overview of Hands-On Experiments, Setting up Development Environment, Dataset Description and Key Performance Indicators for Evaluation</td>
        </tr>
        <tr>
            <td class="tg-0pky">5m</td>
            <td class="tg-0pky">Coffee Break</td>
        </tr>
        <tr>
            <td class="tg-0pky">10m</td>
            <td class="tg-0pky">Experiment 1: Build your Custom Rule-Based Controller</td>
        </tr>
        <tr>
            <td class="tg-0pky">10m</td>
            <td class="tg-0pky">Experiment 2: An Introduction to Tabular Q-Learning Algorithm as an Adaptive Controller</td>
        </tr>
        <tr>
            <td class="tg-0pky">10m</td>
            <td class="tg-0pky">Experiment 3.1: Optimize a Soft-Actor Critic Reinforcement Learning Controller</td>
        </tr>
        <tr>
            <td class="tg-0pky">15m</td>
            <td class="tg-0pky">Experiment 3.2: Tune your Soft-Actor Critic Agent</td>
        </tr>
        <tr>
            <td class="tg-0pky">5m</td>
            <td class="tg-0pky">Coffee Break</td>
        </tr>
        <tr>
            <td class="tg-0pky">20m</td>
            <td class="tg-0pky">Make Submission to The CityLearn Challenge 2023 Control and Forecast Tracks</td>
        </tr>
        <tr>
            <td class="tg-0pky">5m</td>
            <td class="tg-0pky">Next Steps and Concluding Remarks</td>
        </tr>
    </tbody>
</table>

<h2>References</h2>
<ul>
    <li><a href="https://colab.research.google.com/github/intelligent-environments-lab/CityLearn/blob/master/examples/citylearn_rlem23_tutorial.ipynb" target="_blank">CityLearn Tutorial Notebook</a></li>
    <li><a href="https://github.com/intelligent-environments-lab/CityLearn" target="_blank">CityLearn GitHub</a></li>
    <li><a href="https://www.citylearn.net" target="_blank">CityLearn Documentation</a></li>
</ul>