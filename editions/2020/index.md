---
layout: default
title: RLEM Workshop 2020
long_name: "First ACM SIGEnergy Workshop on Reinforcement Learning for Energy Management in Buildings & Cities (RLEM)"
date: November 17th 2020
venue: Yokohama, Japan
description: "RLEM brings together researchers and industry practitioners for the advancement of (deep) reinforcement learning (RL) in the built environment as it is applied for managing energy in civil infrastructure systems (energy, water, transportation)."
---

<html lang="en">
<head>
    <style>
    body{
        background-image: url('{{ site.baseurl }}/assets/images/background_2020.jpeg');
    }
    </style>
</head>
</html>

<h1 class="display-5 mb-4">
    Previous Edition: {{ page.title }}
</h1>
_{{ page.long_name }}_

## About
{{ page.description }}

RLEM'21 will be held in conjunction with [ACM BuildSys'20](http://buildsys.acm.org/2020/). Following BuildSys's directive, the conference will be held virtually on {{ page.date }}.

[Watch](https://youtu.be/OcNSEAujBeA) RLEM'20 recording!

## Registration
[Register now via BuildSys 2020](http://buildsys.acm.org/2020/).

## Important Dates
- __Abstract submission__: August 10, 2020 (AOE)
- __Paper submission__: August 17, 2020 (AOE)
- __Notifications__: September 21, 2020 (AOE)
- __Camera Ready__: October 10, 2020 (AOE)
- __Workshop date__: November 17, 2020

## Call for Papers
Buildings account for 40% of the global energy consumption and 30% ofthe associated greenhouse gas emissions, while also offering a 50–90% CO2 mitigation potential. The transportation sector is responsible for an additional 30%. Optimal decarbonization requires electrification of end-uses a nd concomitant decarbonization of electricity supply, efficient use of electricity for lighting, space heating, cooling and ventilation (HVAC), and domestic hot water generation, and upgrade of the thermal properties of buildings. A major driver for decarbonization are integration of renewable energy systems (RES) into the grid, and photovoltaics (PV) and solar-thermal collectors as well as thermal and electric storage into residential and commercial buildings. Electric vehicles (EVs), with their storage capacity and inherent connectivity, hold a great potential for integration with buildings.
	
The integration of these technologies must be done carefully to unlock their full potential. Artificial intelligence is regarded as a possible pathway to orchestrate these complexities of Smart Cities. In particular, (deep) reinforcement learning algorithms have seen an increased interest and have demonstrated human expert level performance in other domains, e.g., computer games. Research in the building and cities domain has been fragmented and with focus on different problems and using a variety of frameworks. The purpose of this Workshop is to build a growing community around this exciting topic, provide a platform for discussion for future research direction, and share common frameworks.

##### Topics of Interest
Topics of interest include, but are not limited to:
- Challenges and Opportunities for RL in Building and Cities
- Explorations of model vs model-free RL algorithms and hybrids
- Comparisons of RL algorithms to other control solutions, e.g., model-predictive control
- Frameworks and datasets for benchmarking algorithms
- Theoretical contributions to the RL field brought about by constraints/challenges in the buildings/cities domain
- Applications (demand response, HVAC control, occupant integration, traffic scheduling, EV/battery charging, DER integration)

## Submission Instructions
Submitted papers must be unpublished and must not be currently under review for any other publication. Paper submissions must be at most 4 single-spaced US Letter (8.5"x11") pages, including figures, tables, and appendices (excluding references). All submissions must use the LaTeX (preferred) or Word styles found here [https://www.acm.org/publications/proceedings-template](https://www.acm.org/publications/proceedings-template). Authors must make a good faith effort to anonymize their submissions by (1) using the "anonymous" option for the class and (2) using "anonsuppress" section where appropriate. Papers that do not meet the size, formatting, and anonymization requirements will not be reviewed. Please note that ACM uses 9-pt fonts in all conference proceedings, and the style (both LaTeX and Word) implicitly define the font size to be 9-pt.

##### [Submission Link](https://rlem20.hotcrp.com/)

## Program
All times are in Greenwich Meridian Time (GMT).

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-1wig{font-weight:bold;text-align:left;vertical-align:top}
.tg .tg-fymr{border-color:inherit;font-weight:bold;text-align:left;vertical-align:top}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-0lax{text-align:left;vertical-align:top}
.tg .tg-hohi{color:#C9D1D9;text-align:right;vertical-align:top}
.tg .tg-hd9x{color:#C9D1D9;text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-fymr">Time</th>
    <th class="tg-fymr">Session</th>
    <th class="tg-fymr">Title</th>
    <th class="tg-fymr">Speaker</th>
    <th class="tg-fymr">Abstract</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0pky"><span style="font-style:normal;text-decoration:none">12:00-12:20</span></td>
    <td class="tg-0pky" colspan="2"><span style="font-weight:400;font-style:normal;text-decoration:none">Opening remarks</span></td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">General Chair and TPC Chairs</span></td>
    <td class="tg-0pky">-</td>
  </tr>
  <tr>
    <td class="tg-0pky"><span style="font-style:normal;text-decoration:none">12:20-13:00</span></td>
    <td class="tg-0pky">Keynote</td>
    <td class="tg-0pky">Incorporating robust control guarantees within (deep) reinforcement learning</td>
    <td class="tg-0pky"><a href="https://zicokolter.com">Zico Kolter (Associate Professor, Carnegie Mellon University)</a>: Dr Kolter is an Associate Professor in the Computer Science Department with the School of Computer Science at Carnegie Mellon University. In addition, he also serves as Chief Scientist of AI Research for the Bosch Center for AI (BCAI), working in the Pittsburgh Office. His research group focuses on machine learning, optimization, and control. Specifically, much of the research aims at making deep learning algorithms safer, more robust, and more explainable; to these ends, we have worked on methods for training provably robust deep learning systems, and including more complex “modules” (such as optimization solvers) within the loop of deep architectures. Further focus is on several application domains, with a particular focus on applications in smart energy and sustainability domains.</span></td>
    <td class="tg-0pky">Reinforcement learning methods have produced breakthrough results in recent years, but their application to safety-critical systems has been substantially limited by their lack of guarantees, such as those provided by modern robust control techniques.  In this talk, I will discuss a technique we have recently developed that embeds robustness guarantees inside of arbitrary RL policy classes.  Using this approach, we can build deep RL methods that attain much of the performance advantages of modern deep RL (namely, superior performance in "average case" scenarios), while still maintaining robustness in worst-case adversarial settings.  I will highlight experimental results on several simple control systems highlighting the benefits of the method, in addition to a larger-scale smart grid setting, and end by discussing future directions in this line of work.</td>
  </tr>
  <tr>
    <td class="tg-0pky" rowspan="5"><span style="font-style:normal;text-decoration:none">13:00-13:50</span></td>
    <td class="tg-0pky" rowspan="5"><span style="font-weight:400;font-style:normal;text-decoration:none">Session 1: Demanding Response</span></td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none"><a href="https://doi.org/10.1145/3427773.3427862">Demand Response through Price-setting Multi-agent Reinforcement Learning</a></span></td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">M.H. Christensen, C. Ernewein, P. Pinson (Technical University of Denmark)</span></td>
    <td class="tg-0pky">Price based demand response is a cost-effective way of obtaining flexibility needed in power systems with high penetration of intermittent renewable energy sources. Model-free deep reinforcement learning is proposed as a way to train autonomous agents for enabling buildings to participate in demand response programs as well as coordinating such programs though price setting in a multiagent setup. First, we show price responsive control of buildings with electric heat pumps using deep deterministic policy gradient. Then a coordinating agent is trained to manage a population of buildings by adjusting the price in order to keep the total load from exceeding the available capacity considering also the non-flexible base load.</td>
  </tr>
  <tr>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none"><a href="https://doi.org/10.1145/3427773.3427866">Electricity Pricing aware Deep Reinforcement Learning based Intelligent HVAC Control</a></span></td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">K. Kurte, J. Munk, K. Amasyali, O. Kotevska, R. Smith, H. Zandi (Oak Ridge National Laboratory)</span></td>
    <td class="tg-0pky">Recently, deep reinforcement learning (DRL) based intelligent control of Heating, Ventilation, and Air Conditioning (HVAC) has gained a lot of attention due to DRL's ability to optimally control HVAC for minimizing operational cost while maintaining resident's comfort. The success of such DRL-based techniques largely depends on the articulation of the problem in terms of states, actions, and reward function. Inclusion of the electricity pricing information in the problem formulation can play an important role in saving the cost of HVAC operation. However, less attention has been given in the literature on formulating well-crafted state features based on electricity pricing. In this work, we propose an approach for training the DRL model with a specific focus on feature engineering based on electricity pricing. During training, we generate random but sufficiently realistic electricity price signals so that the pre-trained DRL model is robust and adaptive to the dynamic and variable electricity prices. The validation results are encouraging and show the potential of ≈12%-15% savings in the one day cost of HVAC operation, proving the usefulness of including electricity pricing related features as state features.</td>
  </tr>
  <tr>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none"><a href="https://doi.org/10.1145/3427773.3427869">A Centralised Soft Actor Critic Deep Reinforcement Learning Approach to District Demand Side Management through CityLearn</a></span></td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">A. Kathirgamanathan, K. Twardowski, E. Mangina, D. Finn (University College Dublin)</span></td>
    <td class="tg-0pky">Reinforcement learning is a promising model-free and adaptive controller for demand side management, as part of the future smart grid, at the district level. This paper presents the results of the algorithm that was submitted for the CityLearn Challenge, which was hosted in early 2020 with the aim of designing and tuning a reinforcement learning agent to flatten and smooth the aggregated curve of electrical demand of a district of diverse buildings. The proposed solution secured second place in the challenge using a centralised 'Soft Actor Critic' deep reinforcement learning agent that was able to handle continuous action spaces. The controller was able to achieve an averaged score of 0.967 on the challenge dataset comprising of different buildings and climates. This highlights the potential application of deep reinforcement learning as a plug-and-play style controller, that is capable of handling different climates and a heterogenous building stock, for district demand side management of buildings.</td>
  </tr>
  <tr>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none"><a href="https://doi.org/10.1145/3427773.3427870">Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms on a Building Energy Demand Coordination Task</a></span></td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">G. Dhamankar, J. Vazquez-Canteli, Z. Nagy (The University of Texas at Austin)</span></td>
    <td class="tg-0pky">Periods of high demand for electricity can raise electricity prices for building users. Flattening the electricity demand curve reduces can reduce costs and increase resiliency. We formulate this task as a multi-agent reinforcement learning (MA-RL) problem, to be achieved through demand response and coordination of electricity consuming agents, i.e., buildings. Bechmarks for such MA-RL problems do not exist. Here, we contribute an empirical comparison of three classes of MA-RL algorithms: independent learners, centralized critics with decentralized execution, and value factorization learners. We evaluate these algorithms on an energy coordination task in CityLearn, an Open AI Gym environment. We found independent learners with shaped rewards to be competitive with more complex algorithms. Agents with centralized critics aim to learn a rich joint critic, which may complicate the training process and cause scalability issues. Our findings indicate value factorization learners possess the coordination benefits of centralized critics and match independent learners without individualized reward shaping.</td>
  </tr>
  <tr>
    <td class="tg-0pky" colspan="3">Discussion</td>
  </tr>
  <tr>
    <td class="tg-fymr"><span style="font-weight:400;font-style:normal;text-decoration:none">13:50-14:00</span></td>
    <td class="tg-0pky" colspan="4">Break</td>
  </tr>
  <tr>
    <td class="tg-0pky" rowspan="5"><span style="font-style:normal;text-decoration:none">14:00-14:50</span></td>
    <td class="tg-0pky" rowspan="5"><span style="font-weight:400;font-style:normal;text-decoration:none">Session 2: Clash of Algorithms</span></td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none"><a href="https://doi.org/10.1145/3427773.3427864">Less is More: Simplified State-Action Space for Deep Reinforcement Learning based HVAC Control</a></span></td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">S. Murugensan, Z. Jiang, M. Risbeck, J. Amores, C. Zhang, V. Ramamurti, K. Drees, Y. Lee (Johnson Controls Inc)</span></td>
    <td class="tg-0pky">How do we optimize heating, ventilation and air-conditioning (HVAC) systems for energy cost and occupant comfort? How do we accomplish this in an automated fashion that adapts with time with minimal human intervention? Answers to these questions have tremendous impact on building occupant comfort, building operating costs and, importantly, environmental footprint. Understandably, this topic has received considerable attention from experts both in the industry and the academia. Among these works, deep learning, specifically deep reinforcement learning (DRL) is emerging as a data-driven control strategy without requiring an explicit dynamic model of the system. Another advantage of DRL, when successfully developed, is that it can continue to learn and adapt as the building/HVAC characteristics change with time. DRL agents, however, are challenging to train. On one hand, they may need months or years of training data (sample inefficiency), potentially inconveniencing building occupants and incurring high energy costs for a long time. On the other hand, they may converge to local optima or simply do not converge. This paper highlights one strategy to mitigate some of these challenges. We show that the choice of state and action space is as important as the choice of DRL architectures and neural network training techniques. Specifically, we formulate the HVAC control problem as a Partially Observable Markov Decision Process (POMDP), build a DRL agent using Deep Q Networks (DQN) on a building simulator, and quantify gains over a widely adopted baseline heuristic method. Subsequently, we reformulate the original problem as a restricted POMDP by severely restricting the observation (state space) and action space, and build a DRL agent for the restricted POMDP. The performance gains from this DRL agent is double that of the original agent, implying that complex state and action spaces, while information rich, can lead to complex loss functions that could not be maneuvered well by a DRL agent. Our larger message is this: 'less' (state-action space) can be 'more', in the context of DRL training.</td>
  </tr>
  <tr>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none"><a href="https://doi.org/10.1145/3427773.3427867">Continual adaptation in deep reinforcement learning-based control applied to non-stationary building environments</a></span></td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">A. Naug, M. Quinones, G. Biswas (Vanderbilt University)</span></td>
    <td class="tg-0pky">This paper develops a continual deep reinforcement relearning (RL) controller for large buildings that exhibit non-stationary behaviors. The non-stationarity in building operations caused by unexpected changes in weather patterns, occupancy, and faults, makes it imperative to develop control algorithms that adapt to the changing conditions. Given the slow time constants in building operations, we assume that the non-stationarity can be modeled as discrete transitions between stationary models of system behavior. We address the challenge of detecting transitions between stationary processes using trend analysis, and relearn control policies that accommodate the tradeoff between energy savings and comfort after such transitions occur. We demonstrate our approach on a "smart building test-bed" for developing data-driven HVAC controllers that are deployed in large buildings on our university campus.</td>
  </tr>
  <tr>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none"><a href="https://doi.org/10.1145/3427773.3427872">A Comparison of Model-Free and Model Predictive Control for Price Responsive Water Heaters</a></span></td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">D. Biagioni, X. Zhang, P. Graf, D. Sigler, W. Jones (National Renewable Energy Laboratory)</span></td>
    <td class="tg-0pky">We present a careful comparison of two model-free control algorithms, Evolution Strategies (ES) and Proximal Policy Optimization (PPO), with receding horizon model predictive control (MPC) for operating simulated, price responsive water heaters. Four MPC variants are considered: a one-shot controller with perfect forecasting yielding optimal control; a limited-horizon controller with perfect forecasting; a mean forecasting-based controller; and a two-stage stochastic programming controller using historical scenarios. In all cases, the MPC model for water temperature and electricity price are exact; only water demand is uncertain. For comparison, both ES and PPO learn neural network-based policies by directly interacting with the simulated environment under the same scenarios used by MPC. All methods are then evaluated on a separate one-week continuation of the demand time series. We demonstrate that optimal control for this problem is challenging, requiring more than 8-hour lookahead for MPC with perfect forecasting to attain the minimum cost. Despite this challenge, both ES and PPO learn good general purpose policies that outperform mean forecast and two-stage stochastic MPC controllers in terms of average cost and are more than two orders of magnitude faster at computing actions. We show that ES in particular can leverage parallelism to learn a policy in under 90 seconds using 1150 CPU cores.</td>
  </tr>
  <tr>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none"><a href="https://doi.org/10.1145/3427773.3427873">Flexible Reinforcement Learning Framework for Building Control using EnergyPlus-Modelica Energy Models</a></span></td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">J. Lee, S. Huang, A. Rahman, A. Smith, S. Katipamula (Pacific Northwest National Laboratory)</span></td>
    <td class="tg-0pky">In recent years, reinforcement learning (RL) methods have been greatly enhanced by leveraging deep learning approaches. RL methods applied to building control have shown potential in many applications because of their ability to complement or replace conventional methods such as model-based or rule-based controls. However, RL-based building control software is likely tailored either to one target building system or to a specific RL method so that significant additional effort would be required to customize the RL-based controller for use in other building systems or with other RL approaches. Also, RL-based building controls usually depend on building energy simulations to train controllers, so emulating building dynamics (i.e., thermal dynamics and control dynamics) and capturing sub-hourly dynamic profiles are crucial to further the development of effective RL-based building control methods. To address these challenges, we present an RL-based control software employing a high-fidelity hybrid EnergyPlus-Modelica building energy model that emulates building dynamics at 1 minute resolution. This software consists of decoupled components (environment, building emulator, control agent, and RL algorithm), which allows for quick prototyping and benchmarking of standard RL algorithms in different systems; for example, a single component can be replaced without revising the software. To demonstrate this software framework, we conducted a benchmark study using an EnergyPlus-Modelica building energy model for a Chicago office building with an RL-based controller to dynamically control the chilled water temperature setpoint and the air handling unit supply air temperature setpoints.</td>
  </tr>
  <tr>
    <td class="tg-0pky" colspan="3">Discussion</td>
  </tr>
  <tr>
    <td class="tg-fymr"><span style="font-weight:400;font-style:normal;text-decoration:none">14:50-15:00</span></td>
    <td class="tg-0pky" colspan="4">Break</td>
  </tr>
  <tr>
    <td class="tg-0pky" rowspan="5"><span style="font-style:normal;text-decoration:none">19:00-20:20</span></td>
    <td class="tg-0pky" rowspan="5"><span style="font-weight:400;font-style:normal;text-decoration:none">Session 3: Keeping it ReaL</span></td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none"><a href="https://doi.org/10.1145/3427773.3427863">Augmenting Reinforcement Learning with a Planning Model for Optimizing Energy Demand Response in a Prospective Experiment</a></span></td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">L. Spangher, A. Gokul, M. Khattar, J. Palakapilly, A. Tawade, U. Agwan, C. Spanos (University of California, Berkeley)</span></td>
    <td class="tg-0pky">While reinforcement learning (RL) on humans has shown incredible promise, it often suffers from a scarcity of data and few steps. In instances like these, a planning model of human behavior may greatly help. We present an experimental setup for the development and testing of an Soft Actor Critic (SAC) V2 RL architecture for several different neural architectures for planning models: an autoML optimized LSTM, an OLS, and a baseline model. We present the effects of including a planning model in agent learning within a simulation of the office, currently reporting a limited success with the LSTM.</td>
  </tr>
  <tr>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none"><a href="https://doi.org/10.1145/3427773.3427865">Transferable Reinforcement Learning for Smart Homes</a></span></td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">X. Zhang, X. Jin, C. Tripp, D. Biagioni, P. Graf, H. Jiang (National Renewable Energy Laboratory)</span></td>
    <td class="tg-0pky">To harness the great amount of untapped resources on the demand side, smart home technology plays a vital role in solving the "last mile" problem in smart grid. Reinforcement learning (RL), which has demonstrated an outstanding performance in solving many sequential decision-making problems, can be a great candidate to be used in smart home control. For instance, many studies have started investigating the appliance scheduling problem under dynamic pricing scheme. Based on those, this study aims at providing an affordable solution to encourage a higher smart home adoption rate. Specifically, we investigate combining transfer learning (TL) with RL to reduce the training cost of an optimal RL control policy. Given an optimal policy for a benchmark home, TL can jump-start the RL training of a policy for a new home, which has different appliances and user preferences. Simulation results show that by leveraging TL, RL training converges faster and requires much less computing time for new homes that are similar to the benchmark home. In all, this study proposes a cost-effective approach for training RL control policies for homes at scale, which ultimately reduces the controller's implementation costs, increases the adoption rate of RL controllers, and makes more homes grid-interactive.</td>
  </tr>
  <tr>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none"><a href="https://doi.org/10.1145/3427773.3427868">Deep Reinforcement Learning in Buildings: Implicit Assumptions and their Impact</a></span></td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">A. Prakash, S. Touzani, M. Kiran, S. Agarwal, M. Pritoni, J. Granderson (Lawrence Berkeley National Laboratory)</span></td>
    <td class="tg-0pky">As deep reinforcement learning (DRL) continues to gain interest in the smart building research community, there is a transition from simulation-based evaluations to deploying DRL control strategies in actual buildings. While the efficacy of a solution could depend on a particular implementation, there are common obstacles that developers have to overcome to deliver an effective controller. Additionally, a deployment in a physical building can invalidate some of the assumptions made during the controller development. Assumptions on the sensor placement or on the equipment behavior can quickly come undone. This paper presents some of the significant assumptions made during the development of DRL based controllers that could affect their operations in a physical building. Furthermore, a preliminary evaluation revealed that controllers developed with some of these assumptions can incur twice the expected costs when they are deployed in a building.</td>
  </tr>
  <tr>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none"><a href="https://doi.org/10.1145/3427773.3427871">Towards Off-policy Evaluation as a Prerequisite for Real-world Reinforcement Learning in Building Control</a></span></td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">B. Chen (Carnegie Mellon University), M. Jin (Virginia Tech), Z. Wang, T. Hong (Lawrence Berkeley National Laboratory), M. Berges (Carnegie Mellon University)</span></td>
    <td class="tg-0pky">We present an initial study of off-policy evaluation (OPE), a problem prerequisite to real-world reinforcement learning (RL), in the context of building control. OPE is the problem of estimating a policy's performance without running it on the actual system, using historical data from the existing controller. It enables the control engineers to ensure a new, pretrained policy satisfies the performance requirements and safety constraints of a real-world system, prior to interacting with it. While many methods have been developed for OPE, no study has evaluated which ones are suitable for building operational data, which are generated by deterministic policies and have limited coverage of the state-action space. After reviewing existing works and their assumptions, we adopted the approximate model (AM) method. Furthermore, we used bootstrapping to quantify uncertainty and correct for bias. In a simulation study, we evaluated the proposed approach on 10 policies pretrained with imitation learning. On average, the AM method estimated the energy and comfort costs with 1.84% and 14.1% error, respectively.</td>
  </tr>
  <tr>
    <td class="tg-0pky" colspan="3">Discussion</td>
  </tr>
  <tr>
    <td class="tg-0pky"><span style="font-style:normal;text-decoration:none">15:50-16:00</span></td>
    <td class="tg-0pky" colspan="2"><span style="font-weight:400;font-style:normal;text-decoration:none">Closing remarks</span></td>
    <td class="tg-0pky"><span style="font-weight:400;font-style:normal;text-decoration:none">General Chair and TPC Chairs</span></td>
    <td class="tg-0pky">-</td>
  </tr>
  <tr>
    <td class="tg-0pky" colspan="5"><span style="font-weight:400;font-style:normal;text-decoration:none">Happy Hour</span></td>
  </tr>
</tbody>
</table>

## Organization
##### General Chairs
1. [Zoltan Nagy](https://www.caee.utexas.edu/people/faculty/faculty-directory/nagy) (University of Texas at Austin)

##### Technical Program Committee Co-Chairs
1. [Mario Berges](https://inferlab.org/author/mario-berges/) (Carnegie Mellon University)
2. [Bingqing Chen](https://inferlab.org/author/bingqing-chen/) (Carnegie Mellon University)
2. [June Young Park](http://blog.uta.edu/parkjy/) (University of Texas at Arlington)

<!-- ##### Web Chair
1. [Matias Quintana](https://matiasquintana.com/) (National University of Singapore) -->

##### Technical Program Committee
1. [Henning Lange](https://amath.washington.edu/people/henning-lange) (University of Washington)
1. [Helia Zandi](https://www.ornl.gov/staff-profile/helia-zandi) (Oak Ridge National Laboratory)
1. [Jose Vazquez-Canteli](http://nagy.caee.utexas.edu) (University of Texas at Austin)
1. [Zhe Wang](https://buildings.lbl.gov/people/zhe-walter-wang) (Lawrence Berkeley National Lab)
1. [Duc Van Le]() (NTU Singapore)
1. [Wan Du]() (University of California, Merced)
1. [Xin Jin](https://www.nrel.gov/research/staff/xin-jin.html) (National Renewable Energy Laboratory)
1. [Ming Jin]() (University of California, Berkeley)
1. [Alex Vlachokostas]() (Pacific Northwest National Laboratory)
1. [Hari Prasanna Das](http://hariprasanna.com/) (UC Berkeley)
1. [Lucas Spangheer]() (University of California, Berkeley)
1. [Kuldeep Kurte](https://www.ornl.gov/staff-profile/kuldeep-r-kurte) (Oak Ridge National Laboratory)
1. [Ross May]() (Dalarna University)

## Location
{{ page.title}} will be held virtually while ACM BuildSys'20 is located at [{{ page.venue }}](https://buildsys.acm.org/2020/venue/).